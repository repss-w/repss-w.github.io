<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>4th Vision-based Remote Physiological Signal Sensing (RePSS) Challenge & Workshop</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: Arial, sans-serif;
        }
        
        body {
            background-color: #f5f5f5;
        }
        
        .navbar {
            background-color: #000;
            padding: 15px 50px;
            display: flex;
            justify-content: flex-end;
        }
        
        .navbar a {
            color: #fff;
            text-decoration: none;
            margin-left: 20px;
            font-size: 16px;
        }
        
        .navbar a:hover {
            text-decoration: underline;
        }

	.navbar a.active {
            font-weight: bold;
            border-bottom: 2px solid #fff;
        }
        
.hero {
            background-color: #666;
            color: white;
            padding: 50px 0;
            display: flex;
            align-items: center;
            justify-content: center;
            background-image: linear-gradient(rgba(0,0,0,0.5), rgba(0,0,0,0.5)), url('pic.bmp');
            background-size: cover;
            background-position: center;
        }
	.hero-text-content {
            margin-right: 40px;
        }
	    
        .hero-content {
            display: flex;
            max-width: 1200px;
            margin: 0 auto;
            align-items: center;
	justify-content: flex-end;
        }
        
        .logo {
           width: 200px;
            margin-left: 40px; 
        }
        .hero-text {
            display: flex; 
            align-items: center;
            text-align: center; 
        }
        
        .hero-text h1 {
            font-size: 36px;
            margin-bottom: 20px;
            font-weight: bold;
        }
        
        .hero-text p {
            font-size: 20px;
            margin-top: 20px;
        }
        
        .hero-text a {
            color: #fff;
            text-decoration: underline;
        }
        
        .content {
            max-width: 1100px;
            margin: 50px auto;
            padding: 0 20px;
        }
        
        .content h2 {
            font-size: 28px;
            margin-bottom: 20px;
            color: #333;
        }
        
        .content p {
            font-size: 16px;
            line-height: 1.6;
            margin-bottom: 20px;
            color: #333;
        }
        
        .content a {
            color: #333;
            text-decoration: underline;
        }
        
        .content ul {
            margin-left: 40px;
            margin-bottom: 20px;
        }
        
        .content li {
            margin-bottom: 10px;
        }

        .image-gallery {
            margin: 40px 0;
        }

        .image-gallery h2 {
            text-align: center;
            margin-bottom: 30px;
        }

        .gallery-container {
            display: flex;
            flex-direction: row;
            justify-content: space-between;
            gap: 15px;
            flex-wrap: wrap;
        }

        .gallery-item {
            flex: 1;
            min-width: calc(25% - 15px);
            max-width: calc(25% - 15px);
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
            transition: transform 0.3s ease;
        }

        .gallery-item:hover {
            transform: translateY(-5px);
        }

        .gallery-item img {
            width: 100%;
            height: 180px;
            display: block;
            object-fit: cover;
        }

.speakers-section {
            margin: 50px 0;
        }
        
        .speaker-card {
            display: flex;
            margin-bottom: 50px;
            align-items: flex-start;
        }
        
        .speaker-image {
            flex: 0 0 370px;
            margin-right: 30px;
        }
        
        .speaker-image img {
            width: 100%;
            display: block;
            border-radius: 5px;
        }
        
        .speaker-info {
            flex: 1;
        }
        
        .speaker-name {
            text-align: center;
            margin-top: 15px;
            font-size: 22px;
            font-weight: bold;
        }
        
        .speaker-affiliation {
            text-align: center;
            margin-top: 10px;
            font-size: 18px;
            color: #555;
        }
        
        .speaker-details h3 {
            font-size: 18px;
            font-weight: bold;
            margin-bottom: 10px;
            color: #333;
        }
        
        .speaker-details p {
            margin-bottom: 20px;
            line-height: 1.6;
        }


    </style>
</head>
<body>
    <nav class="navbar">
        <a href="index.html">Home</a>
        <a href="workshop.html">Workshop</a>
        <a href="event_schedule.html">Event schedule</a>
        <a href="challenge.html">Challenge</a>
        <a href="organizers.html">Organizers</a>
    </nav>
    
    <div class="hero">
        <div class="hero-content">

            <div class="hero-text">
		<br />
		    <br />
		 
		    <div class="hero-text-content">
                <h1>The 4th RePSS - Multimodal Fusion Learning for Remote Physiological Signal Sensing</h1>
                <p>To be held at <a href="https://2025.ijcai.org/">IJCAI 2025</a>, August 2025, Montreal, Canada</p>
		    </div>
		    <img src="logo.png" alt="RePSS Logo" class="logo"> 
            </div>
        </div>
    </div>
    
    <div class="content">
	<h2>Invited Speakers</h2>
      <!-- <p>TBD</p> -->
        <div class="speakers-section">
            <div class="speaker-card">
                <div class="speaker-image">
                    <img src="https://user.eng.umd.edu/~minwu/photo/wm_web11.jpg" alt="">
                    <div class="speaker-name">Juan Cheng</div>
                    <div class="speaker-affiliation">Hefei University of Technology</div>
                </div>
                <div class="speaker-info">
                    <div class="speaker-details">
                        <h3>Title: Remote HRV Monitoring via Long-Range BVP Signal Recovery</h3>
                        <p></p>
                        
                        <h3>Abstract:</h3>
                        <p>Remote photoplethysmography (rPPG) has shown significant potential in heart rate variability (HRV) estimation. HRV reflects more nuanced autonomic regulation than average heart rate and supports diverse applications, including autonomic function assessment, stress evaluation, and sleep monitoring. Accurate HRV estimation critically depends on recovering long-range, continuous, and high-quality blood volume pulse (BVP) signals from facial videos. This talk introduces a solution for generating high-quality BVP signals of flexible duration from facial videos for accurate HRV monitoring. The proposed method includes a global spatio-temporal modeling strategy for long-range physiological signals, combined with computational efficiency improvements achieved through progressive dimensionality reduction and sparse self-attention.
Finally, we will discuss the future trends of accurate HRV monitoring based on rPPG technology.</p>
                        
                        <h3>Bio:</h3>
 			<p>Juan Cheng received the B.S. and PhD. degrees from the Department of Electronic Science and Technology, University of Science and Technology of China, in 2008 and 2013, respectively. She is currently a full Professor with the Department of Biomedical Engineering, Hefei University of Technology. Her research interests include biomedical signal/image processing and remote intelligent health monitoring. She has published over 60 papers in international journals with 4800+ citations in Google Scholar, and is serving as an Associate Editor for IEEE Signal Processing Letters. She was listed among the world's top 2% of scientists in 2024.</p>
                    </div>
                </div>
            </div>
        </div>
	<br />
	        <div class="speakers-section">
            <div class="speaker-card">
                <div class="speaker-image">
                    <img src="https://user.eng.umd.edu/~minwu/photo/wm_web11.jpg" alt="">
                    <div class="speaker-name">Jingang Shi</div>
                    <div class="speaker-affiliation">Xi’an Jiaotong University</div>
                </div>
                <div class="speaker-info">
                    <div class="speaker-details">
                        <h3>Title: Towards real-world remote physiological signal measurement</h3>
                        <p></p>
                        
                        <h3>Abstract:</h3>
                        <p>In recent years, remote physiological signal measurement has attracted significant attention due to its convenience and effectiveness. It has made substantial progress in applications such as invasive healthcare applications. However, when applied to complex real-world scenarios, the accuracy of the measurements is often challenged by two major factors: video degradation and domain shift on dynamic environments. As we know, real-world videos are typically affected by various forms of unknown and mixed degradation. Due to the subtle nature of physiological signals, they are particularly susceptible to interference from these degradations. It remains a critical challenge to enhance the robustness of physiological signal extraction under unknown degradation conditions. Meanwhile, real-world environments are highly dynamic and induce domain shift across diverse scenarios. In continuously changing dynamic scenarios, how to reduce the interference of domain shift on heart rate signal extraction, and how to leverage unlabeled data from new domains to improve the cross-domain adaptability of the models, are also pressing issues to be addressed. I will share the recent research works in my group to solve the above challenges and discuss the future trend.</p>
                        
                        <h3>Bio:</h3>
 			<p>Jingang Shi received the B.S. and Ph.D. degrees from the Department of Electronics and Information Engineering, Xi’an Jiaotong University, China in 2007 and 2015, respectively. From 2017 to 2020, he was a Postdoctoral Researcher with the Center for Machine Vision and Signal Analysis, University of Oulu, Finland. Since 2020, he has been an Associate Professor with the School of Software, Xi’an Jiaotong University. He has published over 60 papers in international journals and conferences. He served as the Area Chair for ICME. He also organized workshops about Robust Multimedia Image Understanding and Subtle Visual Computing on international conferences such as ICME and ACM Multimedia. His current research interests mainly include image restoration, affective computing, and biomedical signal processing.</p>
                    </div>
                </div>
            </div>
        </div>
	<h2>Call for papers</h2>
      
      
        <p>The workshop calls for high-quality and original research works.  The topic includes but is not limited to:</p>
	<ol>
		<li>New methodologies and data developed for remote measurement of all kinds of physiological responses of human bodies, e.g., heart rate, heart rate variability, respiration, blood oxygen saturation, blood pressure, and others.</li>
		<li>New databases collected for remote measurement of all kinds of physiological signals and their applications.</li>
		<li>Hardwires/apparatuses/imaging systems developed for the purpose of remote physiological measurement from the face and body.</li>
		<li>Solutions for special challenges involved with remote physiological signal sensing, e.g., low-quality video, poor illumination conditions, motions, data with noisy labels or no labels, etc.</li>
		<li>Application-oriented studies of remote physiological sensing, e.g., for medical assessment in hospitals, for health surveillance at home or in other environments, for emotion assessment in various scenarios like for education, job interview, etc., for security and forensics like liveness detection, biometrics, and video editing/synthesis.</li>
		<li>Multimodal approaches for remote measurement of physiological signals are especially welcomed.</li>
	</ol>
      
	<br />
	<h2>Paper Submission Instructions</h2>
	    <ul>
		    <li>Papers must comply with <a href="https://ceur-ws.org/HOWTOSUBMIT.html">CEURART paper style</a> and paper length should be less than 7 pages.</li>
		    <li>The CEURART template can be found on this <a href="https://it.overleaf.com/latex/templates/template-for-submissions-to-ceur-workshop-proceedings-ceur-ws-dot-org/wqyfdgftmcfw/">Overleaf link.</a></li>
		    <li>The review process is <b>double-blind</b>. Please do not include your identity information in your submitted paper.</li>
		    <li>
			    There are two tracks (challenge track and workshop track) for the paper submission.
			    <ul>
				    <li>For the challenge track, The top-3 teams are encouraged to submit papers to this track (Acceptance depends on the quality of the paper, so it is not guaranteed.). Other teams are also welcome to submit papers. The review criterion is based on both the team ranking and the paper quality.</li>
				    <li>For the workshop track, all work in the topic scope mentioned above is encouraged to submit. The review criterion is based on the paper quality and the relevance to RePSS workshop.</li>
				    
			    </ul>
		    </li>
		    <li>ccepted papers will be included in a volume of the <a href="https://ceur-ws.org/">CEUR Workshop Proceedings</a> (EI-index, JUFO1).</li>
		    <li>The <b>submission link</b> is as follow: <a href="https://chairingtool.com/conferences/ijcai25-w19/main-track?role=author">https://chairingtool.com/conferences/ijcai25-w19/main-track?role=author</a></li>
		    <li><b>At least one of the authors of accepted papers should register for the IJCAI 2025 workshop and be present onsite at the workshop.</b></li>
	    </ul>
<br />
	<h2>Important Date</h2>
	    <p>Paper submission is open.</p>
	    <ul>
		    <li><b>May 31 (23:59, AoE)</b>: Paper submission deadline</li>
		    <li>June 6: Notification to authors</li>
		    <li>June 13: Camera-ready deadline</li>
	    </ul>
	    <p><b>Note: Each paper must be presented on-site by an author/co-author at the conference.</b></p>
        


    </div>

    <script>
	document.addEventListener('DOMContentLoaded', function() {
    
		const currentPage = window.location.pathname.split('/').pop() || 'index.html';
    
    
		const navLinks = document.querySelectorAll('.navbar a');
    
    
		navLinks.forEach(link => {
        
			const linkPage = link.getAttribute('href');
        
        
			if (linkPage === currentPage || 
(currentPage === '' && linkPage === 'index.html') ||
 (currentPage === '/' && linkPage === 'index.html')) {
  
		          link.classList.add('active');
        
			}
    
		});

	});

    </script>
</body>
</html>
